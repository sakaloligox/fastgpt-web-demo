let recognition = null;
let isListening = false;
let isSpeaking = false;
let synthUtterance = null;
let chatHistory = []; // ğŸ§  ä¸Šä¸‹æ–‡ç¼“å­˜
let voiceUnlocked = false;

let chatId = localStorage.getItem("chatId");
if (!chatId) {
  chatId = crypto.randomUUID();  // âœ… ç”Ÿæˆ UUID
  localStorage.setItem("chatId", chatId);  // ä¿å­˜åœ¨æœ¬åœ°ç›´åˆ°å…³é—­æ ‡ç­¾é¡µ
}


const camera = document.getElementById("camera");
const askBtn = document.getElementById("askBtn");
const speechText = document.getElementById("speechText");
const resultText = document.getElementById("result");
const langSelect = document.getElementById("langSelect");
const canvas = document.getElementById("snapshot");


// æ‘„åƒå¤´ä¸éº¦å…‹é£æƒé™
navigator.mediaDevices.getUserMedia({
  video: { facingMode: { exact: "environment" } },
  audio: {
    echoCancellation: true,
    noiseSuppression: true,
    autoGainControl: true
  }
})
.then(stream => {
  camera.srcObject = stream;
  camera.muted = true;
})
.catch(() => {
  return navigator.mediaDevices.getUserMedia({
    video: { facingMode: "user" },
    audio: true
  });
})
.then(stream => {
  if (stream) camera.srcObject = stream;
})
.catch(err => alert("Failed to access camera/mic: " + err));

askBtn.addEventListener("mousedown", handleButtonPress);
askBtn.addEventListener("mouseup", handleButtonRelease);
askBtn.addEventListener("touchstart", e => { e.preventDefault(); handleButtonPress(); });
askBtn.addEventListener("touchend", handleButtonRelease);

function handleButtonPress() {
  // âœ… iOS è¯­éŸ³æ’­æŠ¥æƒé™è§£é”ï¼ˆåªåšä¸€æ¬¡ï¼‰
  if (!voiceUnlocked) {
    if (speechSynthesis.paused) {
      speechSynthesis.resume();
    }
    const testVoice = new SpeechSynthesisUtterance('');
    speechSynthesis.speak(testVoice);
    voiceUnlocked = true;
  }

  if (isSpeaking) {
    stopSpeaking();
  } else if (!isListening) {
    startListening();
  }
}

function handleButtonRelease() {
  if (isListening) {
    stopListening();
  }
}

function startListening() {
  isListening = true;
  askBtn.innerText = "â¹ï¸ Listening...";
  speechText.innerText = "";
  resultText.innerText = "";

  canvas.width = 160;
  canvas.height = 120;
  canvas.getContext("2d").drawImage(camera, 0, 0, 160, 120);
  const imageBase64 = canvas.toDataURL("image/jpeg", 0.6);

  recognition = new webkitSpeechRecognition();
  recognition.lang = langSelect.value;
  recognition.interimResults = false;
  recognition.continuous = false;

  recognition.onresult = async (event) => {
    const text = event.results[0][0].transcript;
    speechText.innerText = "You said: " + text;
    isListening = false;
    askBtn.classList.add("loading");
    await sendToFastGPT(imageBase64, text);
  };

  recognition.onerror = () => {
    stopListening();
    speechText.innerText = "(Speech error)";
  };

  recognition.onend = () => {
    if (isListening) {
      stopListening();
      speechText.innerText = "(No speech detected)";
    }
  };

  recognition.start();
  setTimeout(() => {
    if (isListening) {
      recognition.abort();
      stopListening();
      speechText.innerText = "(Timeout: No speech)";
    }
  }, 5000);
}

function stopListening() {
  if (recognition) {
    recognition.abort();
    recognition = null;
  }
  isListening = false;
  if (!isSpeaking) askBtn.innerText = "ğŸ¤ Hold to Speak";
}

function stopSpeaking() {
  speechSynthesis.cancel();
  isSpeaking = false;
  askBtn.innerText = "ğŸ¤ Hold to Speak";
  startListening();
}

async function sendToFastGPT(imageBase64, text) {
  // âœ… æ„å»ºå½“å‰æé—®ï¼ˆä¸åŠ å…¥å†å²ï¼‰
  const currentMessage = [{
    role: "user",
    content: [
      { type: "text", text: text },
      { type: "image_url", image_url: { url: imageBase64 } }
    ]
  }];

  const response = await fetch("/api", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      messages: currentMessage,  // âœ… åªå‘é€å½“å‰æ¶ˆæ¯
      chatId: chatId
    })
  });

  const result = await response.json();
  const reply = result.reply;
  chatId = result.chatId || chatId;

  resultText.innerText = reply;

  // âœ… ä»ç„¶ä¿å­˜åˆ° chatHistoryï¼Œç”¨äºç•Œé¢å±•ç¤ºæˆ–æœªæ¥åˆ‡æ¢åŠŸèƒ½
  chatHistory.push({
    role: "user",
    content: [
      { type: "text", text: text },
      { type: "image_url", image_url: { url: imageBase64 } }
    ]
  });

  chatHistory.push({
    role: "assistant",
    content: [{ type: "text", text: reply }]
  });

  isSpeaking = true;
  askBtn.classList.remove("loading");
  askBtn.innerText = "ğŸ”Š Speaking...";
  synthUtterance = new SpeechSynthesisUtterance(reply);
  synthUtterance.lang = detectLanguage(reply);
  synthUtterance.onend = () => {
    isSpeaking = false;
    askBtn.innerText = "ğŸ¤ Hold to Speak";
  };
  speechSynthesis.speak(synthUtterance);
}

function detectLanguage(text) {
  if (/[ã€-ãƒ¿]/.test(text)) return 'ja-JP';
  if (/[ä¸€-é¿¿]/.test(text)) return 'zh-CN';
  return 'en-US';
}

function startNewChat() {
  chatHistory = [];
  resultText.innerText = '';
  speechText.innerText = '';

  // â—ï¸ç”Ÿæˆæ–°çš„ chatId å¹¶ä¿å­˜ï¼ˆä½¿å…¶æˆä¸ºæ–°çš„å¯¹è¯çº¿ç¨‹ï¼‰
  chatId = crypto.randomUUID();
  localStorage.setItem("chatId", chatId);
}


// iOSè¯­éŸ³è§£é”
function unlockVoicePlayback() {
  const utter = new SpeechSynthesisUtterance('');
  window.speechSynthesis.speak(utter);
  document.removeEventListener("touchstart", unlockVoicePlayback);
  document.removeEventListener("click", unlockVoicePlayback);
}
document.addEventListener("touchstart", unlockVoicePlayback);
document.addEventListener("click", unlockVoicePlayback);
